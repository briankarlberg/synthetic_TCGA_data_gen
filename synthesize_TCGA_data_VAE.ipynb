{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6466e8e5-0d5c-4da2-b53c-acf814234243",
   "metadata": {},
   "source": [
    "Summary:\n",
    "The Cancer Genome Atlas provides high dimensional multi-omic characterizations of tumor resections yet with limited sample sizes, on the order of 100 or less samples for certain tissues-of-origin. Generation of quality synthetic TCGA omic samples can benefit the development of machine learning subtype prediction methods. Traditional methods of generating synthetic data, such as UMAP, can benchmark development of sample generation methods that learn a joint-distribution such as a VAE. Here, TCGA omic data for 995 BRCA samples labeled with one of four subtypes were subset to 20,531 gene-expression features that were then embeded with a VAE into a latent space dimension of four features. The VAE was trained on all four BRCA subtypes simultaneously to generate the feature embedding. The data split used a test set ratio of 0.25. The trained encoder was then used to predict the four latent feauture values for each of the 746 training samples. The average values of 2500 pairwise combinations within each latent feature within each subtype was calcuated to generate a synthetic latent space representation of archetypal samples for each BRCA TCGA subtype. These four latent feature spaces corresponding to each subtype were then decoded back to the 20,531 dimension gene space using the trained decoder component of the VAE. Frequency analysis of selected gene features from these synthetic TCGA data can then leverage the statistical power increase of generative modeling to identify subtype-specific genes of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e86eb146-a9cd-4d01-8334-8074bfb37f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2022-08-18'\n",
    "version = 'synth_TCGA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc3a456-2aad-4559-a1cb-959095b6ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d3e37f-00da-497c-8413-bf0655c64561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../gexp_files/BRCA_gxp.tsv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BRCA run, step 1, point to subsetted gexp data, 995 samples by 20,531\n",
    "gxp_files = sorted(glob.glob('../gexp_files/*.tsv'))\n",
    "gxp_files[2] # <--- set cancer file n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7079f22c-6ba1-40b8-8f21-5b9011837c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file read, comment off for devel\n"
     ]
    }
   ],
   "source": [
    "# BRCA run, step 2, format each expression row as an image sample, scale, x-frm\n",
    "for i in [gxp_files[2]]:\n",
    "    # file  = pd.read_csv(i,         # <--- read cancer file n, comment out for devel reruns\n",
    "    #                    sep = '\\t',\n",
    "    #                    index_col = 0)\n",
    "    print('file read, comment off for devel')\n",
    "    train, test = train_test_split(file)\n",
    "    \n",
    "    trn_empty = [] # append model-formatted numpy arrays of gene expression values\n",
    "    trn_index_key = []\n",
    "    trn_y_list = []\n",
    "    \n",
    "    for j, sample in enumerate(train.index):\n",
    "        gene_row = list(train.iloc[j,1:])\n",
    "        base = np.array(gene_row + (\n",
    "            20736 - len(              # dimension of nearest perfect square   \n",
    "                gene_row)) * [0],\n",
    "                        dtype=np.uint8).reshape(144,144,1)\n",
    "        trn_empty.append(base)\n",
    "        y = train.iloc[j,0]\n",
    "        trn_y_list.append(y)\n",
    "        trn_index_key.append(sample)\n",
    "    formatted = np.array(trn_empty)\n",
    "    formatted = formatted/255 # train normalization\n",
    "    X_train_new = formatted\n",
    "\n",
    "    tst_empty = []\n",
    "    tst_index_key = []\n",
    "    tst_y_list = []\n",
    "    \n",
    "    for j, sample in enumerate(test.index):\n",
    "        gene_row = list(test.iloc[j,1:])\n",
    "        base = np.array(gene_row + (\n",
    "            20736 - len(\n",
    "                gene_row)) * [0],\n",
    "                        dtype=np.uint8).reshape(144,144,1)\n",
    "        tst_empty.append(base)\n",
    "        y = test.iloc[j,0]\n",
    "        tst_y_list.append(y)\n",
    "        tst_index_key.append(sample)\n",
    "    formatted = np.array(tst_empty)\n",
    "    formatted = formatted/255 # test normalization\n",
    "    X_test_new = formatted\n",
    "        \n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf8783c8-d35c-4cfa-a87e-530533331c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(746, 144, 144, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb740fb-8429-4e90-975f-62c54b11c070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249, 144, 144, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b54cdb64-dc63-4a12-a5e4-dc89639ef930",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8fb3ec4-0b53-4d4d-b62e-d02bc544b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an image VAE with Keras\n",
    "# from https://becominghuman.ai/using-variational-autoencoder-vae-to-generate-new-images-14328877e88d\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf; tf.compat.v1.disable_eager_execution()\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Flatten, Lambda, Reshape\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy\n",
    "# import mnist\n",
    "np.random.seed(25)\n",
    "tf.executing_eagerly()\n",
    "\n",
    "def compute_latent(x):\n",
    "    batch = K.shape(mu)[0] # mu and sigma defined after encoder is built\n",
    "    dim = K.int_shape(mu)[1]\n",
    "    eps = K.random_normal(shape=(batch,dim))\n",
    "    return mu + K.exp(sigma/2)*eps\n",
    "\n",
    "def kl_reconstruction_loss(true, pred):\n",
    "    reconstruction_loss = binary_crossentropy(K.flatten(true), K.flatten(pred)) * img_width * img_height\n",
    "    kl_loss = 1 + sigma - K.square(mu) - K.exp(sigma) # mu and sigma input to loss function\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    return K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "img_height   = X_train_new.shape[1]\n",
    "img_width    = X_train_new.shape[2]\n",
    "num_channels = X_train_new.shape[3]\n",
    "input_shape =  (img_height, img_width, num_channels)\n",
    "\n",
    "encoder_input = Input(shape=input_shape)\n",
    "encoder_conv = Conv2D(filters=8, kernel_size=3, strides=2, \n",
    "                padding='same', activation='relu')(encoder_input)\n",
    "encoder_conv = Conv2D(filters=16, kernel_size=3, strides=2,\n",
    "                padding='same', activation='relu')(encoder_input) # Overwrite (?)\n",
    "encoder = Flatten()(encoder_conv)                                 # see model.summary() in slides\n",
    "\n",
    "mu = Dense(latent_dim)(encoder)     # mu defined\n",
    "sigma = Dense(latent_dim)(encoder)  # sigma defined\n",
    "                                                                # 2 args to compute lambda\n",
    "latent_space = Lambda(compute_latent, output_shape=(latent_dim,))([mu, sigma])\n",
    "                    \n",
    "conv_shape = K.int_shape(encoder_conv)\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "decoder = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation='relu')(decoder_input)\n",
    "decoder = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(decoder)\n",
    "decoder_conv = Conv2DTranspose(filters=16, kernel_size=3, strides=2, \n",
    "                           padding='same', activation='relu')(decoder)\n",
    "decoder_conv = Conv2DTranspose(filters=8, kernel_size=3, strides=2, \n",
    "                           padding='same', activation='relu')(decoder)\n",
    "decoder_conv =  Conv2DTranspose(filters=num_channels, kernel_size=3, \n",
    "                          padding='same', activation='sigmoid')(decoder_conv)\n",
    "encoder = Model(encoder_input, latent_space)\n",
    "decoder = Model(decoder_input, decoder_conv)\n",
    "vae = Model(encoder_input, decoder(encoder(encoder_input)))\n",
    "vae.compile(optimizer='adam', loss=kl_reconstruction_loss)  # blank model set for (pre)training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "006fb114-3db3-42b0-a661-684ed1200edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 32 # Number of forward / back pass cycles to set model params with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da696dbf-9c7c-46ba-8115-3d6741c83bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 746 samples, validate on 249 samples\n",
      "Epoch 1/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 13:39:07.167709: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-18 13:39:07.193164: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736/746 [============================>.] - ETA: 0s - loss: 13282.1316"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karlberb/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "746/746 [==============================] - 3s 5ms/sample - loss: 13268.4289 - val_loss: 12123.1658\n",
      "Epoch 2/32\n",
      "746/746 [==============================] - 3s 4ms/sample - loss: 11746.8617 - val_loss: 11344.2825\n",
      "Epoch 3/32\n",
      "746/746 [==============================] - 3s 4ms/sample - loss: 11091.0538 - val_loss: 10834.0824\n",
      "Epoch 4/32\n",
      "746/746 [==============================] - 3s 4ms/sample - loss: 10691.5880 - val_loss: 10549.6990\n",
      "Epoch 5/32\n",
      "746/746 [==============================] - 3s 5ms/sample - loss: 10474.7792 - val_loss: 10394.2416\n",
      "Epoch 6/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 10350.6333 - val_loss: 10300.0038\n",
      "Epoch 7/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 10271.7490 - val_loss: 10236.7187\n",
      "Epoch 8/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 10219.6292 - val_loss: 10193.7691\n",
      "Epoch 9/32\n",
      "746/746 [==============================] - 3s 5ms/sample - loss: 10181.8030 - val_loss: 10161.8202\n",
      "Epoch 10/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 10153.4445 - val_loss: 10134.9446\n",
      "Epoch 11/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 10131.4638 - val_loss: 10117.5981\n",
      "Epoch 12/32\n",
      "746/746 [==============================] - 4s 6ms/sample - loss: 10115.6049 - val_loss: 10099.0239\n",
      "Epoch 13/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 10099.5064 - val_loss: 10093.5525\n",
      "Epoch 14/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 10085.6964 - val_loss: 10081.1703\n",
      "Epoch 15/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 10073.9997 - val_loss: 10062.1884\n",
      "Epoch 16/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 10063.0683 - val_loss: 10051.1788\n",
      "Epoch 17/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 10049.4161 - val_loss: 10039.2746\n",
      "Epoch 18/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 10036.5767 - val_loss: 10039.0309\n",
      "Epoch 19/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 10027.2900 - val_loss: 10015.8726\n",
      "Epoch 20/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 10009.7543 - val_loss: 10003.8789\n",
      "Epoch 21/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 9995.0001 - val_loss: 9987.6147\n",
      "Epoch 22/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 9980.3984 - val_loss: 9976.0096\n",
      "Epoch 23/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 9967.8673 - val_loss: 9966.5832\n",
      "Epoch 24/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 9957.7039 - val_loss: 9957.6878\n",
      "Epoch 25/32\n",
      "746/746 [==============================] - 4s 5ms/sample - loss: 9948.6034 - val_loss: 9947.6042\n",
      "Epoch 26/32\n",
      "746/746 [==============================] - 3s 4ms/sample - loss: 9940.3449 - val_loss: 9941.6899\n",
      "Epoch 27/32\n",
      "746/746 [==============================] - 3s 5ms/sample - loss: 9933.3549 - val_loss: 9936.4722\n",
      "Epoch 28/32\n",
      "746/746 [==============================] - 3s 5ms/sample - loss: 9925.6712 - val_loss: 9928.2868\n",
      "Epoch 29/32\n",
      "746/746 [==============================] - 3s 5ms/sample - loss: 9916.4406 - val_loss: 9923.4169\n",
      "Epoch 30/32\n",
      "746/746 [==============================] - 3s 5ms/sample - loss: 9910.2750 - val_loss: 9919.3212\n",
      "Epoch 31/32\n",
      "746/746 [==============================] - 3s 5ms/sample - loss: 9904.2780 - val_loss: 9917.1976\n",
      "Epoch 32/32\n",
      "746/746 [==============================] - 3s 5ms/sample - loss: 9900.3301 - val_loss: 9913.8013\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TCGA VAE training function\n",
    "history = vae.fit(x=X_train_new, y=X_train_new, epochs=epochs,\n",
    "                                                  # Here is where the validation data goes\n",
    "                  batch_size=32, validation_data=(X_test_new, X_test_new))\n",
    "\n",
    "# Separate plotting from training here            <------- plot only devel break point\n",
    "plt.plot(history.history['loss'],label=\"loss\")\n",
    "plt.plot(history.history['val_loss'],label=\"val_loss\")\n",
    "plt.title('VAE embedding loss, '+\n",
    "          file.index.name+', latent dim = '+str(latent_dim))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.annotate('VAE layer type: Conv2dTranspose\\n'\\\n",
    "                'dtype: uint8, normalized\\ntest_size = .25',\n",
    "            xy=(.4, .8), xycoords='figure fraction',\n",
    "            horizontalalignment='left', verticalalignment='top',\n",
    "            # fontsize=20\n",
    "            )\n",
    "\n",
    "plt.legend(loc=\"lower left\")\n",
    "# plt.savefig('dir_n/cohort_n'+str(epochs)+'_epochs_2022-08-date_version.png')\n",
    "plt.savefig('TCGA_VAE_out/'+\n",
    "            file.index.name+'_'+\n",
    "            str(epochs)+'_epochs_'+\n",
    "            str(latent_dim)+'_latent_dim_'+\n",
    "            date+'_'+version+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e3c9a7a-36df-4b8b-89d0-10ebdde9fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and prep latent feature space for subtype-specfic latent feature generation\n",
    "encoded = encoder.predict(X_train_new)\n",
    "encDF = pd.DataFrame(encoded)\n",
    "encDF.insert(0, 'Labels', trn_y_list) # Adding back labls to subset on subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34e9229d-b058-4c88-8f0f-347e7a0c0d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([0, 1, 2, 3], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_n.columns[1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85adaf34-dd41-47f5-93ca-5b190bfc8f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3.897506\n",
       "2      5.087799\n",
       "3      5.709345\n",
       "4      3.817804\n",
       "7      3.722259\n",
       "         ...   \n",
       "738    6.795300\n",
       "742    6.748266\n",
       "743    3.437369\n",
       "744    4.897128\n",
       "745    3.538478\n",
       "Name: 0, Length: 401, dtype: float32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "496df9b2-0a49-41ff-b046-8318d48ca805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  0,   2,   3,   4,   7,   9,  10,  12,  13,  14,\n",
       "            ...\n",
       "            730, 731, 732, 733, 734, 738, 742, 743, 744, 745],\n",
       "           dtype='int64', length=401)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.index # skipping, index from befor train / test split is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0ae54ed-061c-4989-be54-bc8df9a8d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a550fc0e-470b-48f0-b59c-8dae2cc86c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05ea5bc6-b072-4e50-b0a3-239bc02502f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdx = random.sample(list(col.index), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "922db150-b505-4171-acd8-8648bc89b5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[556]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ac6ad1d4-b348-4420-8cb3-1b235d38f737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(random_samples_per_latent_feature*\n",
    "random_pairs_per_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1c9f0acc-d503-441d-9194-7d16d675dcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRCA_1\n",
      "starting column  0\n",
      "column run time:  0.6757609844207764\n",
      " \n",
      "starting column  1\n",
      "column run time:  0.634835958480835\n",
      " \n",
      "starting column  2\n",
      "column run time:  0.6436009407043457\n",
      " \n",
      "starting column  3\n",
      "column run time:  0.6373608112335205\n",
      " \n",
      "output dictionary built for this subtype\n",
      "2500 k samples generated for this subtype, converting 2D arrays back to rows\n",
      "starting row converter\n",
      "row converter run time 7.280970096588135\n",
      "file write time 61.396124839782715\n",
      "BRCA_2\n",
      "starting column  0\n",
      "column run time:  0.5926320552825928\n",
      " \n",
      "starting column  1\n",
      "column run time:  0.6469550132751465\n",
      " \n",
      "starting column  2\n",
      "column run time:  0.6508030891418457\n",
      " \n",
      "starting column  3\n",
      "column run time:  0.5940961837768555\n",
      " \n",
      "output dictionary built for this subtype\n",
      "2500 k samples generated for this subtype, converting 2D arrays back to rows\n",
      "starting row converter\n",
      "row converter run time 7.095122814178467\n",
      "file write time 61.66805076599121\n",
      "BRCA_3\n",
      "starting column  0\n",
      "column run time:  0.6130020618438721\n",
      " \n",
      "starting column  1\n",
      "column run time:  0.6161611080169678\n",
      " \n",
      "starting column  2\n",
      "column run time:  0.6033411026000977\n",
      " \n",
      "starting column  3\n",
      "column run time:  0.5986049175262451\n",
      " \n",
      "output dictionary built for this subtype\n",
      "2500 k samples generated for this subtype, converting 2D arrays back to rows\n",
      "starting row converter\n",
      "row converter run time 7.2462499141693115\n",
      "file write time 63.42982721328735\n",
      "BRCA_4\n",
      "starting column  0\n",
      "column run time:  0.6099028587341309\n",
      " \n",
      "starting column  1\n",
      "column run time:  0.6054482460021973\n",
      " \n",
      "starting column  2\n",
      "column run time:  0.6033129692077637\n",
      " \n",
      "starting column  3\n",
      "column run time:  0.6182262897491455\n",
      " \n",
      "output dictionary built for this subtype\n",
      "2500 k samples generated for this subtype, converting 2D arrays back to rows\n",
      "starting row converter\n",
      "row converter run time 7.958997011184692\n",
      "file write time 62.865018129348755\n"
     ]
    }
   ],
   "source": [
    "# Take average of random latent feature pairs to generate synthetic samples\n",
    "random_samples_per_latent_feature = 50\n",
    "random_pairs_per_samples = 50\n",
    "for sb_typ in sorted(encDF.Labels.unique()):\n",
    "    # break\n",
    "    print(sb_typ)\n",
    "    sub_n = encDF[encDF.Labels == sb_typ] # subset on a subtype\n",
    "    \n",
    "    sub_start = time.time()\n",
    "    \n",
    "    synth_TCGA_latent_sub_n = {} # replace with numpy array to directly poplulate with synth samples (?)\n",
    "    \n",
    "    for i in sub_n.columns[1:]: # Table of latent feature values for subtype_n, want exhaustive iteration\n",
    "        print('starting column ', i)\n",
    "        \n",
    "        col = sub_n.loc[:,i]\n",
    "\n",
    "        synthetic_latent_feature_vals = [] # want 5k or so\n",
    "        \n",
    "        col_start = time.time()\n",
    "        # for j, ndx in enumerate(col.index): # sample loop, replace with random sampling\n",
    "        for j in list(range(0, random_samples_per_latent_feature)):\n",
    "            rdx = random.sample(list(col.index), 1)\n",
    "             # break # Break stepping through each sample's latent feature value within this column\n",
    "               \n",
    "            # val = col[ndx] # Take a value from this latent feature column\n",
    "            # col_not = col.drop(ndx)\n",
    "            val = col[rdx]   \n",
    "            col_not = col.drop(rdx)\n",
    "                      \n",
    "            pair_val_list = [] # all pairs for the 'val' latent feature for this sample\n",
    "            # for ndx_not in col_not.index: # Observation: taking the difference of vals seems to tighten the joint dist\n",
    "            for k in list(range(0,random_pairs_per_samples)):\n",
    "                rdx_not = random.sample(list(col_not.index), 1)\n",
    "                      \n",
    "                # val_not = col_not[ndx_not]\n",
    "                val_not = col_not[rdx_not]      \n",
    "                      \n",
    "                # print(val, val_not)\n",
    "                new_val = (val.values + val_not.values) / 2 # Generate a synthetic feature value\n",
    "                pair_val_list.append(new_val[0]) # synthetic feature values for this one latent feature in this one column\n",
    "\n",
    "                # break\n",
    "            synthetic_latent_feature_vals = synthetic_latent_feature_vals + pair_val_list\n",
    "        \n",
    "        col_end = time.time() - col_start\n",
    "        print('column run time: ', col_end)\n",
    "        print(' ')\n",
    "        # break # Break column loop\n",
    "        \n",
    "        synth_TCGA_latent_sub_n[str(i)+'_latent'] = synthetic_latent_feature_vals\n",
    "    \n",
    "    print('output dictionary built for this subtype')\n",
    "    # break # break subtype loop\n",
    "    \n",
    "    synth_TCGA_latent_sub_nDF = pd.DataFrame(synth_TCGA_latent_sub_n)\n",
    "    \n",
    "    sub_end = time.time() - sub_start\n",
    "    \n",
    "    synth_TCGA = decoder.predict(np.array(synth_TCGA_latent_sub_nDF))\n",
    "    print((random_samples_per_latent_feature*\n",
    "          random_pairs_per_samples),\n",
    "          'k samples generated for this subtype,'\\\n",
    "          ' converting 2D arrays back to rows')\n",
    "    flat = {}\n",
    "    idx = -1\n",
    "    \n",
    "    print('starting row converter')\n",
    "    row_time = time.time()\n",
    "    for i in synth_TCGA: # all samples of same subtype at this point\n",
    "        list_out = []\n",
    "        for out_val in i.reshape(20736):\n",
    "            list_out.append(out_val)\n",
    "        idx = idx+1 # sample index\n",
    "        flat[idx] = list_out\n",
    "    print('row converter run time', time.time()-row_time)    \n",
    "        \n",
    "    flatDF = pd.DataFrame(flat)\n",
    "    flatDF = flatDF.T\n",
    "    \n",
    "    write_start = time.time()\n",
    "    flatDF.to_csv('synth_TCGA/flat_out_'+sb_typ+'.tsv',\n",
    "                 sep = '\\t')\n",
    "    # np.save('synth_TCGA/synth_'+sb_typ+'.npy', synth_TCGA)\n",
    "    write_end = time.time() - write_start\n",
    "    print('file write time '+str(write_end))\n",
    "    print(' ')\n",
    "    # break # n = 4 BRCA subtype loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1859bb6a-fd67-437a-8d04-69264b10b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_read = pd.read_csv('synth_TCGA/flat_out_BRCA_1.tsv',\n",
    "                       sep = '\\t',\n",
    "                       index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6d694559-2b5b-4cab-ad0a-a56e904da956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20726</th>\n",
       "      <th>20727</th>\n",
       "      <th>20728</th>\n",
       "      <th>20729</th>\n",
       "      <th>20730</th>\n",
       "      <th>20731</th>\n",
       "      <th>20732</th>\n",
       "      <th>20733</th>\n",
       "      <th>20734</th>\n",
       "      <th>20735</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>0.040120</td>\n",
       "      <td>0.489319</td>\n",
       "      <td>0.487461</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.477514</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.002065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009801</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.038367</td>\n",
       "      <td>0.470500</td>\n",
       "      <td>0.503215</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.480132</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.001752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013625</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.048297</td>\n",
       "      <td>0.472674</td>\n",
       "      <td>0.501233</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.471887</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.006209</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.002374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015521</td>\n",
       "      <td>0.004915</td>\n",
       "      <td>0.052540</td>\n",
       "      <td>0.472192</td>\n",
       "      <td>0.497709</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.469988</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.007208</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.005533</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.002760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013582</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.046844</td>\n",
       "      <td>0.482375</td>\n",
       "      <td>0.496553</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.477280</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.006904</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.002668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>0.029314</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.086576</td>\n",
       "      <td>0.458798</td>\n",
       "      <td>0.521895</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.493945</td>\n",
       "      <td>0.008631</td>\n",
       "      <td>0.027257</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.007856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>0.023572</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>0.073058</td>\n",
       "      <td>0.466547</td>\n",
       "      <td>0.514546</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.482912</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>0.015067</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>0.010694</td>\n",
       "      <td>0.009725</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.005357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>0.015925</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>0.053583</td>\n",
       "      <td>0.478395</td>\n",
       "      <td>0.504549</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.480052</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.006041</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.003347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>0.016172</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.055713</td>\n",
       "      <td>0.467221</td>\n",
       "      <td>0.510979</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.482407</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.010404</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.006317</td>\n",
       "      <td>0.005471</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.003338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>0.015116</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.052202</td>\n",
       "      <td>0.471878</td>\n",
       "      <td>0.507182</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.486113</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.011009</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.004304</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.003309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 20736 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.011428  0.003253  0.040120  0.489319  0.487461  0.000409  0.477514   \n",
       "1     0.009801  0.002687  0.038367  0.470500  0.503215  0.000352  0.480132   \n",
       "2     0.013625  0.004187  0.048297  0.472674  0.501233  0.000576  0.471887   \n",
       "3     0.015521  0.004915  0.052540  0.472192  0.497709  0.000679  0.469988   \n",
       "4     0.013582  0.004086  0.046844  0.482375  0.496553  0.000596  0.477280   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2495  0.029314  0.011216  0.086576  0.458798  0.521895  0.002691  0.493945   \n",
       "2496  0.023572  0.008568  0.073058  0.466547  0.514546  0.001698  0.482912   \n",
       "2497  0.015925  0.005102  0.053583  0.478395  0.504549  0.000840  0.480052   \n",
       "2498  0.016172  0.005220  0.055713  0.467221  0.510979  0.000874  0.482407   \n",
       "2499  0.015116  0.004701  0.052202  0.471878  0.507182  0.000788  0.486113   \n",
       "\n",
       "             7         8         9  ...     20726     20727     20728  \\\n",
       "0     0.001920  0.004227  0.000056  ...  0.000139  0.000421  0.000208   \n",
       "1     0.001727  0.006070  0.000047  ...  0.000107  0.000266  0.000163   \n",
       "2     0.002658  0.006209  0.000062  ...  0.000171  0.000421  0.000263   \n",
       "3     0.003304  0.007208  0.000078  ...  0.000223  0.000566  0.000328   \n",
       "4     0.002609  0.006904  0.000088  ...  0.000199  0.000520  0.000295   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2495  0.008631  0.027257  0.000558  ...  0.000999  0.001562  0.001358   \n",
       "2496  0.005904  0.015067  0.000246  ...  0.000549  0.001010  0.000805   \n",
       "2497  0.003255  0.008653  0.000117  ...  0.000262  0.000593  0.000397   \n",
       "2498  0.003483  0.010404  0.000122  ...  0.000272  0.000558  0.000407   \n",
       "2499  0.003233  0.011009  0.000135  ...  0.000266  0.000580  0.000388   \n",
       "\n",
       "         20729     20730     20731     20732     20733     20734     20735  \n",
       "0     0.000771  0.003387  0.002449  0.000999  0.000148  0.000079  0.002065  \n",
       "1     0.000602  0.003042  0.002407  0.000795  0.000155  0.000056  0.001752  \n",
       "2     0.000912  0.004983  0.004157  0.001385  0.000194  0.000105  0.002374  \n",
       "3     0.001121  0.005533  0.004518  0.001652  0.000214  0.000129  0.002760  \n",
       "4     0.001023  0.004449  0.003437  0.001325  0.000246  0.000114  0.002668  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2495  0.003483  0.013514  0.012292  0.004663  0.001545  0.000581  0.007856  \n",
       "2496  0.002271  0.010694  0.009725  0.003350  0.000768  0.000360  0.005357  \n",
       "2497  0.001280  0.006041  0.005059  0.001777  0.000370  0.000166  0.003347  \n",
       "2498  0.001284  0.006317  0.005471  0.001814  0.000397  0.000163  0.003338  \n",
       "2499  0.001252  0.005277  0.004304  0.001570  0.000404  0.000146  0.003309  \n",
       "\n",
       "[2500 rows x 20736 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "54fcefbd-9d6c-47a0-ba86-8f5d9c0441f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdevel\u001b[m\u001b[m/               flat_out_BRCA.tsv    flat_out_BRCA_1.tsv\n"
     ]
    }
   ],
   "source": [
    "ls synth_TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "868ff496-6d36-4ca4-9260-32243c30ba8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20726</th>\n",
       "      <th>20727</th>\n",
       "      <th>20728</th>\n",
       "      <th>20729</th>\n",
       "      <th>20730</th>\n",
       "      <th>20731</th>\n",
       "      <th>20732</th>\n",
       "      <th>20733</th>\n",
       "      <th>20734</th>\n",
       "      <th>20735</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>0.040120</td>\n",
       "      <td>0.489319</td>\n",
       "      <td>0.487461</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.477514</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.002065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009801</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.038367</td>\n",
       "      <td>0.470499</td>\n",
       "      <td>0.503215</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.480132</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.001752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013625</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.048297</td>\n",
       "      <td>0.472674</td>\n",
       "      <td>0.501233</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.471887</td>\n",
       "      <td>0.002658</td>\n",
       "      <td>0.006209</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.002374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015521</td>\n",
       "      <td>0.004915</td>\n",
       "      <td>0.052540</td>\n",
       "      <td>0.472192</td>\n",
       "      <td>0.497709</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.469988</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.007208</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.005533</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.002760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013582</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.046844</td>\n",
       "      <td>0.482375</td>\n",
       "      <td>0.496552</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.477280</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.006904</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.002668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>0.029314</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.086576</td>\n",
       "      <td>0.458798</td>\n",
       "      <td>0.521895</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.493945</td>\n",
       "      <td>0.008631</td>\n",
       "      <td>0.027257</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.007856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>0.023572</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>0.073058</td>\n",
       "      <td>0.466547</td>\n",
       "      <td>0.514546</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.482912</td>\n",
       "      <td>0.005904</td>\n",
       "      <td>0.015067</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>0.010694</td>\n",
       "      <td>0.009725</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.005357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>0.015925</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>0.053583</td>\n",
       "      <td>0.478395</td>\n",
       "      <td>0.504549</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.480052</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.006041</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.003347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>0.016172</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.055713</td>\n",
       "      <td>0.467221</td>\n",
       "      <td>0.510979</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.482407</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.010404</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.006317</td>\n",
       "      <td>0.005471</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.003338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>0.015116</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.052202</td>\n",
       "      <td>0.471878</td>\n",
       "      <td>0.507182</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.486113</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.011009</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.004304</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.003309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 20736 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6      \\\n",
       "0     0.011428  0.003253  0.040120  0.489319  0.487461  0.000409  0.477514   \n",
       "1     0.009801  0.002687  0.038367  0.470499  0.503215  0.000352  0.480132   \n",
       "2     0.013625  0.004187  0.048297  0.472674  0.501233  0.000576  0.471887   \n",
       "3     0.015521  0.004915  0.052540  0.472192  0.497709  0.000679  0.469988   \n",
       "4     0.013582  0.004086  0.046844  0.482375  0.496552  0.000596  0.477280   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2495  0.029314  0.011216  0.086576  0.458798  0.521895  0.002691  0.493945   \n",
       "2496  0.023572  0.008568  0.073058  0.466547  0.514546  0.001698  0.482912   \n",
       "2497  0.015925  0.005102  0.053583  0.478395  0.504549  0.000840  0.480052   \n",
       "2498  0.016172  0.005220  0.055713  0.467221  0.510979  0.000874  0.482407   \n",
       "2499  0.015116  0.004701  0.052202  0.471878  0.507182  0.000788  0.486113   \n",
       "\n",
       "         7         8         9      ...     20726     20727     20728  \\\n",
       "0     0.001920  0.004227  0.000056  ...  0.000139  0.000421  0.000208   \n",
       "1     0.001727  0.006070  0.000047  ...  0.000107  0.000266  0.000163   \n",
       "2     0.002658  0.006209  0.000062  ...  0.000171  0.000421  0.000263   \n",
       "3     0.003304  0.007208  0.000078  ...  0.000223  0.000566  0.000328   \n",
       "4     0.002609  0.006904  0.000088  ...  0.000199  0.000520  0.000295   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2495  0.008631  0.027257  0.000558  ...  0.000999  0.001562  0.001358   \n",
       "2496  0.005904  0.015067  0.000246  ...  0.000549  0.001010  0.000805   \n",
       "2497  0.003255  0.008653  0.000117  ...  0.000262  0.000593  0.000397   \n",
       "2498  0.003483  0.010404  0.000122  ...  0.000272  0.000558  0.000407   \n",
       "2499  0.003233  0.011009  0.000135  ...  0.000266  0.000580  0.000388   \n",
       "\n",
       "         20729     20730     20731     20732     20733     20734     20735  \n",
       "0     0.000771  0.003387  0.002449  0.000999  0.000148  0.000079  0.002065  \n",
       "1     0.000602  0.003042  0.002407  0.000795  0.000155  0.000056  0.001752  \n",
       "2     0.000912  0.004983  0.004157  0.001385  0.000194  0.000105  0.002374  \n",
       "3     0.001121  0.005533  0.004518  0.001652  0.000214  0.000129  0.002760  \n",
       "4     0.001023  0.004449  0.003437  0.001325  0.000246  0.000114  0.002668  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2495  0.003483  0.013514  0.012292  0.004663  0.001545  0.000581  0.007856  \n",
       "2496  0.002271  0.010694  0.009725  0.003350  0.000768  0.000360  0.005357  \n",
       "2497  0.001280  0.006041  0.005059  0.001777  0.000370  0.000166  0.003347  \n",
       "2498  0.001284  0.006317  0.005471  0.001814  0.000397  0.000163  0.003338  \n",
       "2499  0.001252  0.005277  0.004304  0.001570  0.000404  0.000146  0.003309  \n",
       "\n",
       "[2500 rows x 20736 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc25fa-a757-4421-92c6-8a10f304374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c9f91e7f-cc8d-410c-b2ce-9e5d683c6e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 144, 144, 1)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_TCGA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a4f708fb-e2e5-4627-b7b4-b27ac3d83a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(synth_TCGA_latent_sub_nDF).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "60d8fab5-529c-4a50-9baf-4f44bbb6c91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.2167635,  1.8178809, -5.758805 ,  4.8147635],\n",
       "       [ 5.33401  ,  1.1863337, -5.9859824,  3.9155443],\n",
       "       [ 4.428418 , -0.5677565, -5.35021  ,  4.036207 ],\n",
       "       ...,\n",
       "       [ 4.564437 ,  0.6278343, -3.0039349,  3.0404325],\n",
       "       [ 6.048405 ,  1.7043012, -4.6700754,  3.0127733],\n",
       "       [ 4.888307 ,  2.3172226, -4.9403305,  4.386728 ]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(synth_TCGA_latent_sub_nDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5fcf00a8-4e92-4925-a62f-8d8ab2dad65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karlberb/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "synth_TCGA = decoder.predict(np.array(synth_TCGA_latent_sub_nDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7aa8d-67e4-4aa7-9b8e-74acf5a0b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write-out synthetic samples prior to prediction (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5840b854-a086-4746-bb3c-a047a82dec77",
   "metadata": {},
   "source": [
    "The following set of cells compare the mean and variance of the actual latent values of the 401 BRCA_1 training samples and 10k latent values of the synthesized BRCA_1 samples. Averging the pairwise values 'tightens' the distribution by picking a point always closer to the mean than the greater value of the pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "52bf7523-3940-4338-bbb4-d41fce79c368",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_n) # BRCA_1 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a97e4482-f725-484f-9f39-839874e8dcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5v/88tvwqd94gs4dfr3h6rt1_w0qd13bg/T/ipykernel_17055/2169004526.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  sub_n.mean(axis = 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    4.958225\n",
       "1    2.302188\n",
       "2   -4.124426\n",
       "3    2.885818\n",
       "dtype: float32"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_n.mean(axis = 0) # mean of real latent features from training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ba7e42ba-6f48-4af6-9867-c44bf5cb4789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(synthetic_latent_feature_vals) # 10k sythetic BRCA_1 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eb405402-c5a6-46be-924d-c64b7bc4c793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0_latent    4.858744\n",
       "1_latent    2.295246\n",
       "2_latent   -3.988985\n",
       "3_latent    2.900736\n",
       "dtype: float32"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_TCGA_latent_sub_nDF.mean(axis = 0) # nearly the same as real latent means above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "46c7da85-6147-41bc-8b27-acdd1c42a3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5v/88tvwqd94gs4dfr3h6rt1_w0qd13bg/T/ipykernel_17055/1052470201.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  sub_n.std(axis = 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1.066332\n",
       "1    1.713051\n",
       "2    1.942558\n",
       "3    1.660366\n",
       "dtype: float32"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_n.std(axis = 0) # stdevs of real latent features from training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a5e3cc0b-4a56-4f8c-9874-c9063f34f774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0_latent    0.721570\n",
       "1_latent    1.206479\n",
       "2_latent    1.394161\n",
       "3_latent    1.071198\n",
       "dtype: float32"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_TCGA_latent_sub_nDF.std(axis = 0) # synthetic latent feature stdevs have tighter spread of values around each mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a18576e4-5151-4508-a43f-9f686d53ebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5v/88tvwqd94gs4dfr3h6rt1_w0qd13bg/T/ipykernel_17055/3114008434.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  sub_n.std(axis = 0).values / synth_TCGA_latent_sub_nDF.std(axis = 0).values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.4777946, 1.4198769, 1.3933529, 1.550009 ], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_n.std(axis = 0).values / synth_TCGA_latent_sub_nDF.std(axis = 0).values\n",
    "# ratio of stdev reduction is relatively consistent, 1.39 to 1.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ced9f5db-4c92-41ea-9386-502d51ad0f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_latent</th>\n",
       "      <th>1_latent</th>\n",
       "      <th>2_latent</th>\n",
       "      <th>3_latent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.216763</td>\n",
       "      <td>1.817881</td>\n",
       "      <td>-5.758805</td>\n",
       "      <td>4.814764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.334010</td>\n",
       "      <td>1.186334</td>\n",
       "      <td>-5.985982</td>\n",
       "      <td>3.915544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.428418</td>\n",
       "      <td>-0.567756</td>\n",
       "      <td>-5.350210</td>\n",
       "      <td>4.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.782153</td>\n",
       "      <td>0.754092</td>\n",
       "      <td>-6.007211</td>\n",
       "      <td>4.227410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.078863</td>\n",
       "      <td>1.925956</td>\n",
       "      <td>-3.742669</td>\n",
       "      <td>4.643601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5.399395</td>\n",
       "      <td>1.380816</td>\n",
       "      <td>-4.345580</td>\n",
       "      <td>3.849928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>4.980171</td>\n",
       "      <td>0.897927</td>\n",
       "      <td>-2.996569</td>\n",
       "      <td>3.811306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>4.564437</td>\n",
       "      <td>0.627834</td>\n",
       "      <td>-3.003935</td>\n",
       "      <td>3.040432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>6.048405</td>\n",
       "      <td>1.704301</td>\n",
       "      <td>-4.670075</td>\n",
       "      <td>3.012773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4.888307</td>\n",
       "      <td>2.317223</td>\n",
       "      <td>-4.940331</td>\n",
       "      <td>4.386728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0_latent  1_latent  2_latent  3_latent\n",
       "0     4.216763  1.817881 -5.758805  4.814764\n",
       "1     5.334010  1.186334 -5.985982  3.915544\n",
       "2     4.428418 -0.567756 -5.350210  4.036207\n",
       "3     5.782153  0.754092 -6.007211  4.227410\n",
       "4     6.078863  1.925956 -3.742669  4.643601\n",
       "...        ...       ...       ...       ...\n",
       "9995  5.399395  1.380816 -4.345580  3.849928\n",
       "9996  4.980171  0.897927 -2.996569  3.811306\n",
       "9997  4.564437  0.627834 -3.003935  3.040432\n",
       "9998  6.048405  1.704301 -4.670075  3.012773\n",
       "9999  4.888307  2.317223 -4.940331  4.386728\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_TCGA_latent_sub_nDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dea1aa77-e2a7-4d77-bb18-ee36a1eec47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_TCGA_latent_sub_nDF = pd.DataFrame(synth_TCGA_latent_sub_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530bf824-b4f1-4e31-80c1-f20b75d7a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devel checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16dda72f-f183-4e66-9961-d19726c4a5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.958225250244141"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c372c95-ddc0-42fd-8302-5283f9fa2a62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.987333"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(synthetic_latent_feature_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "afc8eece-30e8-4b7b-a131-2531c20990df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0663319826126099"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fb3be9ed-f7f0-443e-b573-ed8e0c558c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7608427"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(synthetic_latent_feature_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "61ed883f-c9bb-4b9f-b40e-0dd1fb003638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(synthetic_latent_feature_vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
