{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8209baa6-5d5a-4f09-956b-f98cfd9b62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Want VAE fitting run time comparison\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "\n",
    "import glob as glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Layer\n",
    "from tensorflow.keras import metrics, optimizers\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6170dbf8-a097-4d86-aac0-880f986495cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent compute\n",
    "def compute_latent(x): # x:\n",
    "    mu, sigma = x\n",
    "    batch = K.shape(mu)[0]\n",
    "    dim = K.shape(mu)[1]\n",
    "    eps = K.random_normal(shape=(batch,dim), mean=0., stddev=1.0 )\n",
    "    return mu + K.exp(sigma/2)*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aded6a94-3798-4a1d-b96f-9cdecc8b1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer for loss\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded - K.square(z_mean_encoded) - \n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc26a7d-90a3-449c-b949-b63b54c318b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec9b4ee-01c2-4d2a-8955-f4ec394c329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths_5k = sorted(glob.glob('data_5k/*.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54980c68-4dc1-442d-920f-6f5c58071bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths_5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a566409c-07ec-47f9-9456-d1d823487f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths_5k = sorted(glob.glob('data_5k/*.tsv'))\n",
    "for train_path in train_paths_5k:\n",
    "    train_df = pd.read_csv(train_path, sep=\"\\t\", index_col=0)\n",
    "    train_cohort = train_df.index.name\n",
    "    features = train_df.columns[1:]\n",
    "    \n",
    "    original_dim = len(features)\n",
    "    feature_dim = len(features)\n",
    "    latent_dim = 100\n",
    "    \n",
    "    encoder_inputs = keras.Input(shape=(feature_dim,))\n",
    "    z_mean_dense_linear = layers.Dense(latent_dim, kernel_initializer='glorot_uniform', name=\"encoder_1\")(encoder_inputs)\n",
    "    z_mean_dense_batchnorm = layers.BatchNormalization()(z_mean_dense_linear)\n",
    "    z_mean_encoded = layers.Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "    z_log_var_dense_linear = layers.Dense(latent_dim, kernel_initializer='glorot_uniform', name=\"encoder_2\")(encoder_inputs)\n",
    "    z_log_var_dense_batchnorm = layers.BatchNormalization()(z_log_var_dense_linear)\n",
    "    z_log_var_encoded = layers.Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "    latent_space = layers.Lambda(compute_latent, output_shape=(latent_dim,), name=\"latent_space\")([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "    decoder_to_reconstruct = layers.Dense(feature_dim, kernel_initializer='glorot_uniform', activation='sigmoid')\n",
    "    decoder_outputs = decoder_to_reconstruct(latent_space)\n",
    "\n",
    "    learning_rate = 0.0005\n",
    "    kappa = 1\n",
    "\n",
    "    beta = K.variable(0)\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=learning_rate)\n",
    "    vae_layer = CustomVariationalLayer()([encoder_inputs, decoder_outputs])\n",
    "    vae = Model(encoder_inputs, vae_layer)\n",
    "    vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n",
    "\n",
    "    # X_train = train_df.iloc[:, 1:]\n",
    "\n",
    "    epochs=100\n",
    "\n",
    "    fit_start = time.time()\n",
    "    history = vae.fit(train_df.iloc[:, 1:],  \n",
    "                epochs=epochs, batch_size=50, shuffle=True,\n",
    "                callbacks=[WarmUpCallback(beta, kappa)],\n",
    "                     verbose=0)\n",
    "    fit_end = time.time() - fit_start\n",
    "\n",
    "    plt.plot(history.history['loss'],label=\"loss\")\n",
    "\n",
    "    plt.title(\n",
    "        train_cohort+' loss for direct predict on other cohorts n = 24'+\n",
    "        '\\nTybalt VAE train\\n'\n",
    "             )\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.annotate('Feature set = '+'MAD 5000k'+\n",
    "                 '\\nLatent dim = '+str(latent_dim)+\n",
    "                 '\\nLayer type = dense'+\n",
    "                 '\\nNormalization = divide by max()',\n",
    "                xy=(.4, .8), xycoords='figure fraction',\n",
    "                horizontalalignment='left', verticalalignment='top',\n",
    "                )\n",
    "\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(\n",
    "        'g_devel/RNB00978_runtimes/'+train_cohort+'_loss_'+\n",
    "            str(round(fit_end,2))+'_runtimeRNB00978.png',\n",
    "        bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef8e86-3f0d-41bf-8031-f82b99b243e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
