{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8209baa6-5d5a-4f09-956b-f98cfd9b62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Want VAE fitting run time comparison\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from statistics import mean\n",
    "from statistics import stdev\n",
    "\n",
    "import glob as glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Layer\n",
    "from tensorflow.keras import metrics, optimizers\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6170dbf8-a097-4d86-aac0-880f986495cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent compute\n",
    "def compute_latent(x): # x:\n",
    "    mu, sigma = x\n",
    "    batch = K.shape(mu)[0]\n",
    "    dim = K.shape(mu)[1]\n",
    "    eps = K.random_normal(shape=(batch,dim), mean=0., stddev=1.0 )\n",
    "    return mu + K.exp(sigma/2)*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aded6a94-3798-4a1d-b96f-9cdecc8b1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer for loss\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded - K.square(z_mean_encoded) - \n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc26a7d-90a3-449c-b949-b63b54c318b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec9b4ee-01c2-4d2a-8955-f4ec394c329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths_5k = sorted(glob.glob('data_5k/*.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a566409c-07ec-47f9-9456-d1d823487f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/karlberb/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:Output custom_variational_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 14:07:51.301975: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output custom_variational_layer_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_1.\n",
      "WARNING:tensorflow:Output custom_variational_layer_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_2.\n",
      "WARNING:tensorflow:Output custom_variational_layer_3 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_3.\n",
      "WARNING:tensorflow:Output custom_variational_layer_4 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_4.\n",
      "WARNING:tensorflow:Output custom_variational_layer_5 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_5.\n",
      "WARNING:tensorflow:Output custom_variational_layer_6 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_6.\n",
      "WARNING:tensorflow:Output custom_variational_layer_7 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_7.\n",
      "WARNING:tensorflow:Output custom_variational_layer_8 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_8.\n",
      "WARNING:tensorflow:Output custom_variational_layer_9 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_9.\n",
      "WARNING:tensorflow:Output custom_variational_layer_10 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_10.\n",
      "WARNING:tensorflow:Output custom_variational_layer_11 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_11.\n",
      "WARNING:tensorflow:Output custom_variational_layer_12 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_12.\n",
      "WARNING:tensorflow:Output custom_variational_layer_13 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_13.\n",
      "WARNING:tensorflow:Output custom_variational_layer_14 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_14.\n",
      "WARNING:tensorflow:Output custom_variational_layer_15 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_15.\n",
      "WARNING:tensorflow:Output custom_variational_layer_16 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_16.\n",
      "WARNING:tensorflow:Output custom_variational_layer_17 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_17.\n",
      "WARNING:tensorflow:Output custom_variational_layer_18 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_18.\n",
      "WARNING:tensorflow:Output custom_variational_layer_19 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_19.\n",
      "WARNING:tensorflow:Output custom_variational_layer_20 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_20.\n",
      "WARNING:tensorflow:Output custom_variational_layer_21 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_21.\n",
      "WARNING:tensorflow:Output custom_variational_layer_22 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_22.\n",
      "WARNING:tensorflow:Output custom_variational_layer_23 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_23.\n",
      "WARNING:tensorflow:Output custom_variational_layer_24 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_24.\n"
     ]
    }
   ],
   "source": [
    "train_paths_5k = sorted(glob.glob('data_5k/*.tsv'))\n",
    "for train_path in train_paths_5k:\n",
    "    train_df = pd.read_csv(train_path, sep=\"\\t\", index_col=0)\n",
    "    train_cohort = train_df.index.name\n",
    "    features = train_df.columns[1:]\n",
    "    \n",
    "    original_dim = len(features)\n",
    "    feature_dim = len(features)\n",
    "    latent_dim = 100\n",
    "    \n",
    "    encoder_inputs = keras.Input(shape=(feature_dim,))\n",
    "    z_mean_dense_linear = layers.Dense(latent_dim, kernel_initializer='glorot_uniform', name=\"encoder_1\")(encoder_inputs)\n",
    "    z_mean_dense_batchnorm = layers.BatchNormalization()(z_mean_dense_linear)\n",
    "    z_mean_encoded = layers.Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "    z_log_var_dense_linear = layers.Dense(latent_dim, kernel_initializer='glorot_uniform', name=\"encoder_2\")(encoder_inputs)\n",
    "    z_log_var_dense_batchnorm = layers.BatchNormalization()(z_log_var_dense_linear)\n",
    "    z_log_var_encoded = layers.Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "    latent_space = layers.Lambda(compute_latent, output_shape=(latent_dim,), name=\"latent_space\")([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "    decoder_to_reconstruct = layers.Dense(feature_dim, kernel_initializer='glorot_uniform', activation='sigmoid')\n",
    "    decoder_outputs = decoder_to_reconstruct(latent_space)\n",
    "\n",
    "    learning_rate = 0.0005\n",
    "    kappa = 1\n",
    "\n",
    "    beta = K.variable(0)\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=learning_rate)\n",
    "    vae_layer = CustomVariationalLayer()([encoder_inputs, decoder_outputs])\n",
    "    vae = Model(encoder_inputs, vae_layer)\n",
    "    vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n",
    "\n",
    "    # X_train = train_df.iloc[:, 1:]\n",
    "\n",
    "    epochs=100\n",
    "\n",
    "    fit_start = time.time()\n",
    "    history = vae.fit(train_df.iloc[:, 1:],  \n",
    "                epochs=epochs, batch_size=50, shuffle=True,\n",
    "                callbacks=[WarmUpCallback(beta, kappa)],\n",
    "                     verbose=0)\n",
    "    fit_end = time.time() - fit_start\n",
    "\n",
    "    plt.plot(history.history['loss'],label=\"loss\")\n",
    "\n",
    "    plt.title(\n",
    "        train_cohort+' loss for direct predict on other cohorts n = 24'+\n",
    "        '\\nTybalt VAE train\\n'\n",
    "             )\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.annotate('Feature set = '+'MAD 5000k'+\n",
    "                 '\\nLatent dim = '+str(latent_dim)+\n",
    "                 '\\nLayer type = dense'+\n",
    "                 '\\nNormalization = divide by max()',\n",
    "                xy=(.4, .8), xycoords='figure fraction',\n",
    "                horizontalalignment='left', verticalalignment='top',\n",
    "                )\n",
    "\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(\n",
    "        'RNB00978_runtimes/'+train_cohort+'_loss_'+\n",
    "            str(round(fit_end,2))+'_runtime.png',\n",
    "        bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcef8e86-3f0d-41bf-8031-f82b99b243e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casey_Greene_baseline.ipynb\n",
      "Classical ML on raw vs decoded notes.rtf\n",
      "Code_demo_2022-11-02.rtf\n",
      "J-Lab_Exa_blog.txt\n",
      "\u001b[34mRNB00978_runtimes\u001b[m\u001b[m/\n",
      "a_ntrsct_00.ipynb\n",
      "b_model_trn_one_prdct_anoth_00.ipynb\n",
      "baseline_00.ipynb\n",
      "baseline_01.ipynb\n",
      "baseline_02.ipynb\n",
      "baseline_03.ipynb\n",
      "baseline_04.ipynb\n",
      "baseline_05.ipynb\n",
      "clf_org_dcd_ovrly.ipynb\n",
      "d_org-dcd.ipynb\n",
      "\u001b[34mdata\u001b[m\u001b[m/\n",
      "\u001b[34mdata_5k\u001b[m\u001b[m/\n",
      "\u001b[34mdecoded\u001b[m\u001b[m/\n",
      "\u001b[34mdecoded_baseline_F1s\u001b[m\u001b[m/\n",
      "\u001b[34mdecoded_baseline_plots\u001b[m\u001b[m/\n",
      "mad5k.ipynb\n",
      "matrices_build.ipynb\n",
      "non-RNA.ipynb\n",
      "normalizer.ipynb\n",
      "\u001b[34moriginal_normalized\u001b[m\u001b[m/\n",
      "\u001b[34moverlay_plots\u001b[m\u001b[m/\n",
      "runtime_check-Copy1.ipynb\n",
      "\u001b[34mtransfer_learning_loss_plots\u001b[m\u001b[m/\n",
      "umap.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf1629e-0c91-40cd-85d4-31e7dee5c305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
